{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poetry Generation with RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function from solutions in Set 6 to change text to data\n",
    "def parse_observations(text):\n",
    "    # Convert text to dataset.\n",
    "    lines = [line.split() for line in text.split('\\n') if line.split()]\n",
    "\n",
    "    obs_counter = 0\n",
    "    obs = []\n",
    "    obs_map = {}\n",
    "\n",
    "    for line in lines:\n",
    "        obs_elem = []\n",
    "        \n",
    "        for word in line:\n",
    "            word = re.sub(r'[^\\w]', '', word).lower()\n",
    "            if word not in obs_map:\n",
    "                # Add unique words to the observations map.\n",
    "                obs_map[word] = obs_counter\n",
    "                obs_counter += 1\n",
    "            \n",
    "            # Add the encoded word.\n",
    "            obs_elem.append(obs_map[word])\n",
    "        \n",
    "        # Add the encoded sequence.\n",
    "        obs.append(obs_elem)\n",
    "\n",
    "    return obs, obs_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  98029\n",
      "Total Vocab:  71\n",
      "Total Patterns:  48995\n",
      "48995\n"
     ]
    }
   ],
   "source": [
    "text = open(os.path.join(os.getcwd(), 'data/shakespeare.txt')).read()\n",
    "obs, obs_map = parse_observations(text)\n",
    "\n",
    "#chars = list(text)\n",
    "chars = sorted(list(set(text)))\n",
    "n_chars, n_vocab = len(text), len(chars)\n",
    "\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 40\n",
    "train = []\n",
    "char_seqs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 2):\n",
    "    seq_in = text[i:i + seq_length]\n",
    "    train.append(seq_in)\n",
    "    \n",
    "    if i + seq_length < n_chars - 1:\n",
    "        seq_out = text[i + seq_length]\n",
    "        char_seqs.append(seq_out)\n",
    "\n",
    "n_patterns = len(train)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "char_seqs.append(' ')\n",
    "\n",
    "print(len(char_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One-hot encode the training data\n",
    "\n",
    "xTrain = np.zeros((n_patterns, seq_length, n_vocab))\n",
    "yTrain = np.zeros((n_patterns, n_vocab))\n",
    "\n",
    "for ind in range(n_patterns):\n",
    "    sentence = train[ind]\n",
    "    for i, c in enumerate(sentence):\n",
    "        xTrain[ind, i, char_to_int[c]] = 1\n",
    "    \n",
    "    yTrain[ind, char_to_int[char_seqs[ind]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "48995/48995 [==============================] - 93s - loss: 2.9800    \n",
      "Epoch 2/60\n",
      "48995/48995 [==============================] - 95s - loss: 2.4398    \n",
      "Epoch 3/60\n",
      "48995/48995 [==============================] - 69s - loss: 2.2270    \n",
      "Epoch 4/60\n",
      "48995/48995 [==============================] - 249s - loss: 2.1251   \n",
      "Epoch 5/60\n",
      "48995/48995 [==============================] - 176s - loss: 2.0519   \n",
      "Epoch 6/60\n",
      "48995/48995 [==============================] - 69s - loss: 1.9959    \n",
      "Epoch 7/60\n",
      "48995/48995 [==============================] - 68s - loss: 1.9507    \n",
      "Epoch 8/60\n",
      "48995/48995 [==============================] - 61s - loss: 1.9074    \n",
      "Epoch 9/60\n",
      "48995/48995 [==============================] - 68s - loss: 1.8731    \n",
      "Epoch 10/60\n",
      "48995/48995 [==============================] - 66s - loss: 1.8438    \n",
      "Epoch 11/60\n",
      "48995/48995 [==============================] - 64s - loss: 1.8158    \n",
      "Epoch 12/60\n",
      "48995/48995 [==============================] - 650s - loss: 1.7886   \n",
      "Epoch 13/60\n",
      "48995/48995 [==============================] - 76s - loss: 1.7680    \n",
      "Epoch 14/60\n",
      "48995/48995 [==============================] - 69s - loss: 1.7454    \n",
      "Epoch 15/60\n",
      "48995/48995 [==============================] - 63s - loss: 1.7272    \n",
      "Epoch 16/60\n",
      "48995/48995 [==============================] - 65s - loss: 1.7105    \n",
      "Epoch 17/60\n",
      "48995/48995 [==============================] - 66s - loss: 1.6909    \n",
      "Epoch 18/60\n",
      "48995/48995 [==============================] - 60s - loss: 1.6729    \n",
      "Epoch 19/60\n",
      "48995/48995 [==============================] - 64s - loss: 1.6560    \n",
      "Epoch 20/60\n",
      "48995/48995 [==============================] - 60s - loss: 1.6407    \n",
      "Epoch 21/60\n",
      "48995/48995 [==============================] - 61s - loss: 1.6213    \n",
      "Epoch 22/60\n",
      "48995/48995 [==============================] - 62s - loss: 1.6054    \n",
      "Epoch 23/60\n",
      "48995/48995 [==============================] - 64s - loss: 1.5913    \n",
      "Epoch 24/60\n",
      "48995/48995 [==============================] - 61s - loss: 1.5779    \n",
      "Epoch 25/60\n",
      "48995/48995 [==============================] - 62s - loss: 1.5601    \n",
      "Epoch 26/60\n",
      "48995/48995 [==============================] - 29s - loss: 1.5435    \n",
      "Epoch 27/60\n",
      "48995/48995 [==============================] - 61s - loss: 1.5288    \n",
      "Epoch 28/60\n",
      "48995/48995 [==============================] - 62s - loss: 1.5120    \n",
      "Epoch 29/60\n",
      "48995/48995 [==============================] - 65s - loss: 1.4937    \n",
      "Epoch 30/60\n",
      "48995/48995 [==============================] - 99s - loss: 1.4814    \n",
      "Epoch 31/60\n",
      "48995/48995 [==============================] - 64s - loss: 1.4649    \n",
      "Epoch 32/60\n",
      "48995/48995 [==============================] - 73s - loss: 1.4538    \n",
      "Epoch 33/60\n",
      "48995/48995 [==============================] - 73s - loss: 1.4345    \n",
      "Epoch 34/60\n",
      "48995/48995 [==============================] - 69s - loss: 1.4187    \n",
      "Epoch 35/60\n",
      "48995/48995 [==============================] - 66s - loss: 1.4022    \n",
      "Epoch 36/60\n",
      "48995/48995 [==============================] - 62s - loss: 1.3872    \n",
      "Epoch 37/60\n",
      "48995/48995 [==============================] - 80s - loss: 1.3758    \n",
      "Epoch 38/60\n",
      "48995/48995 [==============================] - 62s - loss: 1.3581    \n",
      "Epoch 39/60\n",
      "48995/48995 [==============================] - 74s - loss: 1.3452    \n",
      "Epoch 40/60\n",
      "48995/48995 [==============================] - 61s - loss: 1.3273    \n",
      "Epoch 41/60\n",
      "48995/48995 [==============================] - 61s - loss: 1.3148    \n",
      "Epoch 42/60\n",
      "48995/48995 [==============================] - 70s - loss: 1.2995    \n",
      "Epoch 43/60\n",
      "48995/48995 [==============================] - 61s - loss: 1.2812    \n",
      "Epoch 44/60\n",
      "48995/48995 [==============================] - 62s - loss: 1.2687    \n",
      "Epoch 45/60\n",
      "48995/48995 [==============================] - 63s - loss: 1.2535    \n",
      "Epoch 46/60\n",
      "48995/48995 [==============================] - 70s - loss: 1.2399    \n",
      "Epoch 47/60\n",
      "48995/48995 [==============================] - 64s - loss: 1.2262    \n",
      "Epoch 48/60\n",
      "48995/48995 [==============================] - 310s - loss: 1.2166   \n",
      "Epoch 49/60\n",
      "48995/48995 [==============================] - 145s - loss: 1.1996   \n",
      "Epoch 50/60\n",
      "48995/48995 [==============================] - 127s - loss: 1.1826   \n",
      "Epoch 51/60\n",
      "48995/48995 [==============================] - 118s - loss: 1.1718   \n",
      "Epoch 52/60\n",
      "48995/48995 [==============================] - 103s - loss: 1.1638   \n",
      "Epoch 53/60\n",
      "48995/48995 [==============================] - 70s - loss: 1.1491    \n",
      "Epoch 54/60\n",
      "48995/48995 [==============================] - 79s - loss: 1.1403    \n",
      "Epoch 55/60\n",
      "48995/48995 [==============================] - 79s - loss: 1.1251    \n",
      "Epoch 56/60\n",
      "48995/48995 [==============================] - 76s - loss: 1.1103    \n",
      "Epoch 57/60\n",
      "48995/48995 [==============================] - 72s - loss: 1.1029    \n",
      "Epoch 58/60\n",
      "48995/48995 [==============================] - 68s - loss: 1.0916    \n",
      "Epoch 59/60\n",
      "48995/48995 [==============================] - 67s - loss: 1.0838    \n",
      "Epoch 60/60\n",
      "48995/48995 [==============================] - 69s - loss: 1.0718    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x109ba8b38>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_length, n_vocab)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_vocab, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "model.fit(xTrain, yTrain, epochs=60, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(a, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature of 0.25\n",
      "shall i compare thee to a summer's day?\n",
      "That hanst thou he wours but for mine, and thou mays,\n",
      "And for the world and care my heart be de,\n",
      "That of this longures dound with the beart\n",
      "That us and in endill be the beart\n",
      "The oroun the sun and they fail as ande\n",
      "With paartion prow my a parting should,\n",
      "And in the canse hath thou art the sull deate,\n",
      "Which my menst for the worts that without,\n",
      "  To she strong tound thy self thy worth dost,\n",
      "I  s a caull I have sweet my meaking eart,\n",
      "  And this this fillice thy eresurn's soull,\n",
      "And thou my love soul fail shall with make\n",
      "And than thou shall with the blewsen ste my,\n",
      "Sive that that I heaven this love the groud,\n",
      "And to bencasse mine one doth fout with doth,\n",
      " \n",
      "\n",
      "\n",
      "Temperature of 0.75\n",
      "shall i compare thee to a summer's day?\n",
      "So far I baros, sheel 'fourn thas frame sold,\n",
      "And hore mine e's now and this linged in fare,\n",
      "  five noth thy live and that by sed ind,\n",
      "But my self it to mun my seef, dosn deess,\n",
      "And you like mine a dofon more shee summ:\n",
      " hauth ast thou mide ofee I when I an ming,\n",
      "  Then I love patter, me connace mo deer,\n",
      "What I brave to meas which I waste soon still,\n",
      "Whou me sech spear to dans by that tone,\n",
      "Themeis thy a foum a nay a deat have,\n",
      "  Thou may a fall stelb, not love to thy deeWhours,\n",
      "Than give worth hase my purting sweet revied.\n",
      "Of bath being this lend, and all my see,\n",
      "Nor deat have so thine  pird the see treing,\n",
      "Which whice thy give aspearing now fair thou\n",
      "\n",
      "\n",
      "Temperature of 1.5\n",
      "shall i compare thee to a summer's day?\n",
      "gher on's loised boughtsur ain.\n",
      "  cante thome dosc dadss than houd lainth\n",
      "I egignthour,\n",
      "vactuch othrent. my benunde ksinge nour dut.\n",
      "\n",
      "\n",
      "                    1rohEve ge mire provilits giknob,\n",
      "To thy woring  wiot till youth natMuld doth part.\n",
      "Mouring o minl wor dosh foaaule what ustoabu.\n",
      "O blick have dare they foom thy faor will in Thoeath\n",
      "-obneat elfore, gever with rot, youn,\n",
      "Hond har's hy hears 'ntreffence it quicil.\n",
      ",\n",
      "  rich uwcany with vircy, I new,\n",
      "  wits me:\n",
      "hy nos srimeds both nster the colfoor roan,\n",
      "x  yours may erof grummosting is thy dilTher,.\n",
      "Why thy beoungase, hus thou nomg dle.\n",
      "\n",
      "\n",
      "                    1yrorswor resh sean con all his wiwr,\n",
      "chawnu\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for temp in [0.25, 0.75, 1.5]:\n",
    "    start = numpy.random.randint(0, n_chars - seq_length - 1)\n",
    "    sentence = \"shall i compare thee to a summer's day?\\n\"\n",
    "    seq_in = sentence\n",
    "    # generate characters\n",
    "    for ind in range(660):\n",
    "        x_format = np.zeros((1, seq_length, n_vocab))\n",
    "\n",
    "        for i, c in enumerate(sentence):\n",
    "            x_format[0, i, char_to_int[c]] = 1.0\n",
    "\n",
    "        prediction = model.predict(x_format, verbose=0)[0]\n",
    "\n",
    "        pred_array = np.array(prediction)\n",
    "\n",
    "        #best_ind = np.argmax(pred_array)\n",
    "\n",
    "        best_ind = sample(pred_array, temp)\n",
    "\n",
    "        result = int_to_char[best_ind]\n",
    "\n",
    "        sentence = sentence[1:] + result\n",
    "\n",
    "        seq_in += result\n",
    "    #print( \"\\nDone.\")\n",
    "    print(\"Temperature of \" + str(temp))\n",
    "    print(seq_in)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
