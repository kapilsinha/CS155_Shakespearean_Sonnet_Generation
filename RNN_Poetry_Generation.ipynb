{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poetry Generation with RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function from solutions in Set 6 to change text to data\n",
    "def parse_observations(text):\n",
    "    # Convert text to dataset.\n",
    "    lines = [line.split() for line in text.split('\\n') if line.split()]\n",
    "\n",
    "    obs_counter = 0\n",
    "    obs = []\n",
    "    obs_map = {}\n",
    "\n",
    "    for line in lines:\n",
    "        obs_elem = []\n",
    "        \n",
    "        for word in line:\n",
    "            word = re.sub(r'[^\\w]', '', word).lower()\n",
    "            if word not in obs_map:\n",
    "                # Add unique words to the observations map.\n",
    "                obs_map[word] = obs_counter\n",
    "                obs_counter += 1\n",
    "            \n",
    "            # Add the encoded word.\n",
    "            obs_elem.append(obs_map[word])\n",
    "        \n",
    "        # Add the encoded sequence.\n",
    "        obs.append(obs_elem)\n",
    "\n",
    "    return obs, obs_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  98029\n",
      "Total Vocab:  71\n",
      "Total Patterns:  48995\n",
      "48995\n"
     ]
    }
   ],
   "source": [
    "text = open(os.path.join(os.getcwd(), 'data/shakespeare.txt')).read()\n",
    "obs, obs_map = parse_observations(text)\n",
    "\n",
    "#chars = list(text)\n",
    "chars = sorted(list(set(text)))\n",
    "n_chars, n_vocab = len(text), len(chars)\n",
    "\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 40\n",
    "train = []\n",
    "char_seqs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 2):\n",
    "    seq_in = text[i:i + seq_length]\n",
    "    train.append(seq_in)\n",
    "    \n",
    "    if i + seq_length < n_chars - 1:\n",
    "        seq_out = text[i + seq_length]\n",
    "        char_seqs.append(seq_out)\n",
    "\n",
    "n_patterns = len(train)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "char_seqs.append(' ')\n",
    "\n",
    "print(len(char_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One-hot encode the training data\n",
    "\n",
    "xTrain = np.zeros((n_patterns, seq_length, n_vocab))\n",
    "yTrain = np.zeros((n_patterns, n_vocab))\n",
    "\n",
    "for ind in range(n_patterns):\n",
    "    sentence = train[ind]\n",
    "    for i, c in enumerate(sentence):\n",
    "        xTrain[ind, i, char_to_int[c]] = 1\n",
    "    \n",
    "    yTrain[ind, char_to_int[char_seqs[ind]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "48995/48995 [==============================] - 72s - loss: 2.9674    \n",
      "Epoch 2/60\n",
      "48995/48995 [==============================] - 115s - loss: 2.4258   \n",
      "Epoch 3/60\n",
      "48995/48995 [==============================] - 84s - loss: 2.2312    \n",
      "Epoch 4/60\n",
      "48995/48995 [==============================] - 82s - loss: 2.1211    \n",
      "Epoch 5/60\n",
      "48995/48995 [==============================] - 70s - loss: 2.0507    \n",
      "Epoch 6/60\n",
      "48995/48995 [==============================] - 79s - loss: 1.9978    \n",
      "Epoch 7/60\n",
      "48995/48995 [==============================] - 59s - loss: 1.9498     \n",
      "Epoch 8/60\n",
      "48995/48995 [==============================] - 110s - loss: 1.9111   \n",
      "Epoch 9/60\n",
      "48995/48995 [==============================] - 96s - loss: 1.8746    \n",
      "Epoch 10/60\n",
      "48995/48995 [==============================] - 72s - loss: 1.8433    \n",
      "Epoch 11/60\n",
      "48995/48995 [==============================] - 79s - loss: 1.8144    \n",
      "Epoch 12/60\n",
      "48995/48995 [==============================] - 106s - loss: 1.7901   \n",
      "Epoch 13/60\n",
      "48995/48995 [==============================] - 77s - loss: 1.7693    \n",
      "Epoch 14/60\n",
      "48995/48995 [==============================] - 113s - loss: 1.7452   \n",
      "Epoch 15/60\n",
      "48995/48995 [==============================] - 94s - loss: 1.7239    \n",
      "Epoch 16/60\n",
      "48995/48995 [==============================] - 76s - loss: 1.7073    \n",
      "Epoch 17/60\n",
      "48995/48995 [==============================] - 96s - loss: 1.6863    \n",
      "Epoch 18/60\n",
      "48995/48995 [==============================] - 82s - loss: 1.6677    \n",
      "Epoch 19/60\n",
      "48995/48995 [==============================] - 68s - loss: 1.6514    \n",
      "Epoch 20/60\n",
      "48995/48995 [==============================] - 99s - loss: 1.6317    \n",
      "Epoch 21/60\n",
      "48995/48995 [==============================] - 92s - loss: 1.6166    \n",
      "Epoch 22/60\n",
      "48995/48995 [==============================] - 75s - loss: 1.6007    \n",
      "Epoch 23/60\n",
      "48995/48995 [==============================] - 73s - loss: 1.5835    \n",
      "Epoch 24/60\n",
      "48995/48995 [==============================] - 107s - loss: 1.5658   \n",
      "Epoch 25/60\n",
      "48995/48995 [==============================] - 67s - loss: 1.5488    \n",
      "Epoch 26/60\n",
      "48995/48995 [==============================] - 74s - loss: 1.5350    \n",
      "Epoch 27/60\n",
      "48995/48995 [==============================] - 62s - loss: 1.5192    \n",
      "Epoch 28/60\n",
      "48995/48995 [==============================] - 65s - loss: 1.5020    \n",
      "Epoch 29/60\n",
      "48995/48995 [==============================] - 65s - loss: 1.4894    \n",
      "Epoch 30/60\n",
      "48995/48995 [==============================] - 1920s - loss: 1.4690  \n",
      "Epoch 31/60\n",
      "48995/48995 [==============================] - 473s - loss: 1.4487   \n",
      "Epoch 32/60\n",
      "48995/48995 [==============================] - 366s - loss: 1.4356   \n",
      "Epoch 33/60\n",
      "48995/48995 [==============================] - 777s - loss: 1.4220   \n",
      "Epoch 34/60\n",
      "48995/48995 [==============================] - 105s - loss: 1.4054   \n",
      "Epoch 35/60\n",
      "48995/48995 [==============================] - 91s - loss: 1.3877    \n",
      "Epoch 36/60\n",
      "48995/48995 [==============================] - 94s - loss: 1.3709    \n",
      "Epoch 37/60\n",
      "48995/48995 [==============================] - 89s - loss: 1.3541    \n",
      "Epoch 38/60\n",
      "48995/48995 [==============================] - 88s - loss: 1.3404    \n",
      "Epoch 39/60\n",
      "48995/48995 [==============================] - 86s - loss: 1.3228    \n",
      "Epoch 40/60\n",
      "48995/48995 [==============================] - 91s - loss: 1.3090    \n",
      "Epoch 41/60\n",
      "48995/48995 [==============================] - 106s - loss: 1.2957   \n",
      "Epoch 42/60\n",
      "48995/48995 [==============================] - 125s - loss: 1.2778   \n",
      "Epoch 43/60\n",
      "48995/48995 [==============================] - 128s - loss: 1.2625   \n",
      "Epoch 44/60\n",
      "48995/48995 [==============================] - 120s - loss: 1.2443   \n",
      "Epoch 45/60\n",
      "48995/48995 [==============================] - 126s - loss: 1.2368   \n",
      "Epoch 46/60\n",
      "48995/48995 [==============================] - 552s - loss: 1.2180   \n",
      "Epoch 47/60\n",
      "48995/48995 [==============================] - 110s - loss: 1.2034   \n",
      "Epoch 48/60\n",
      "48995/48995 [==============================] - 90s - loss: 1.1890    \n",
      "Epoch 49/60\n",
      "48995/48995 [==============================] - 185s - loss: 1.1767   \n",
      "Epoch 50/60\n",
      "48995/48995 [==============================] - 206s - loss: 1.1626   \n",
      "Epoch 51/60\n",
      "48995/48995 [==============================] - 523s - loss: 1.1525   \n",
      "Epoch 52/60\n",
      "48995/48995 [==============================] - 87s - loss: 1.1366    \n",
      "Epoch 53/60\n",
      "48995/48995 [==============================] - 78s - loss: 1.1242    \n",
      "Epoch 54/60\n",
      "48995/48995 [==============================] - 271s - loss: 1.1159   \n",
      "Epoch 55/60\n",
      "48995/48995 [==============================] - 83s - loss: 1.1015     \n",
      "Epoch 56/60\n",
      "48995/48995 [==============================] - 110s - loss: 1.0902   \n",
      "Epoch 57/60\n",
      "48995/48995 [==============================] - 79s - loss: 1.0784    \n",
      "Epoch 58/60\n",
      "48995/48995 [==============================] - 85s - loss: 1.0663    \n",
      "Epoch 59/60\n",
      "48995/48995 [==============================] - 91s - loss: 1.0596    \n",
      "Epoch 60/60\n",
      "48995/48995 [==============================] - 123s - loss: 1.0485   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ca115bda0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_length, n_vocab)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_vocab, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "model.fit(xTrain, yTrain, epochs=60, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "The corell dot my self me sences in thee,\n",
      "  This thing hat I am that I do couse,\n",
      "  Then this should be ithers and true should hath and resile,\n",
      "That hath my love this linge that thou art,\n",
      "And to the eaver beauty of thy san,\n",
      "That thes for when I am the love's foon dure.\n",
      "The brat the surst of hear that fear betred,\n",
      "  in thee to me fan thy self and thing allow,\n",
      "The  we thou dead farth me for my self when self thought\n",
      "To must this should that men mored sweet sor,\n",
      "  And so seef I dear that I wort of thee,\n",
      "The wordot soul stoul sump to thy ure's sing,\n",
      "That have a condounded in the lorst\n",
      "Thou hast a corfon my doth when thou art,\n",
      "Which in thee my self and world with stan,\n",
      "Whon thou art my heart world be thours in the distinge,\n",
      "And that to the ead the world so best inge,\n",
      "If any dost confornt,\n",
      "That there a me my love soul store of thee,\n",
      "  This thinge should this will thou art thee, my dodn and,\n",
      "Time to the world sheer when I am you dot,\n",
      "Then thou art the sear that well thou grace,\n",
      "And with then f\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, n_chars - seq_length - 1)\n",
    "sentence = \"shall i compare thee to a summer's day?\\n\"\n",
    "seq_in = sentence\n",
    "# generate characters\n",
    "for ind in range(1000):\n",
    "    x_format = np.zeros((1, seq_length, n_vocab))\n",
    "    \n",
    "    for i, c in enumerate(sentence):\n",
    "        x_format[0, i, char_to_int[c]] = 1.0\n",
    "    \n",
    "    prediction = model.predict(x_format, verbose=0)\n",
    "    \n",
    "    pred_array = np.array(prediction)\n",
    "    \n",
    "    best_ind = np.argmax(pred_array[0])\n",
    "    \n",
    "    result = int_to_char[best_ind]\n",
    "    \n",
    "    sentence = sentence[1:] + result\n",
    "    \n",
    "    seq_in += result\n",
    "#print( \"\\nDone.\")\n",
    "print(seq_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
